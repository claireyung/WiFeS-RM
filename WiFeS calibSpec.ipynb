{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WiFeS calibSpec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load OzDES_calibSpec_calc.py\n",
    "# ---------------------------------------------------------- #\n",
    "# ----------------- OzDES_calibSpec_calc.py ---------------- #\n",
    "# ------- https://github.com/jhoormann/OzDES_calibSpec ----- #\n",
    "# ---------------------------------------------------------- #\n",
    "# This is a code to perform spectrophotometric calibration.  #\n",
    "# It was designed to calibrate spectral data from the Anglo  #\n",
    "# Australian Telescope by matching it to near simultaneous   #\n",
    "# photometric observations using DECam on the Blanco         #\n",
    "# Telescope as part of the OzDES Reverberation Mapping       #\n",
    "# Program.   It also has the option to coadd all spectra     #\n",
    "# observed either by observing run or by date of observation.#\n",
    "# The bulk of the calculations are defined in the file       #\n",
    "# calibSpec_calc.py.  This code defines file locations,      #\n",
    "# reads in the data, and calls the calibration function.     #\n",
    "# Unless otherwise noted this code was written by            #\n",
    "# Janie Hoormann.                                            #\n",
    "# ---------------------------------------------------------- #\n",
    "\n",
    "# Modified for the WiFeS telescope by Claire Yung            #\n",
    "\n",
    "from astropy.io import fits\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.interpolate import InterpolatedUnivariateSpline\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "\n",
    "from scipy.spatial.distance import pdist, cdist, squareform\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor, kernels\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------- #\n",
    "# Modified from a function originally provided by    #\n",
    "# Anthea King                                        #\n",
    "# -------------------------------------------------- #\n",
    "# ------------------ Spectrumv18 ------------------- #\n",
    "# -------------------------------------------------- #\n",
    "# Read in spectral data assuming the format from v18 #\n",
    "# of the OzDES reduction pipeline. Modify if your    #\n",
    "# input data is stored differently                   #\n",
    "# -------------------------------------------------- #\n",
    "\n",
    "class Spectrumv18(object):\n",
    "    def __init__(self, filepath=None):\n",
    "        assert filepath is not None\n",
    "        self.filepath = filepath\n",
    "        try:\n",
    "            self.data = fits.open(filepath)\n",
    "        except IOError:\n",
    "            print(\"Error: file {0} could not be found\".format(filepath))\n",
    "            exit()\n",
    "        data = fits.open(filepath)\n",
    "        self.combinedFlux = data[1]\n",
    "        self.combinedVariance = data[2]\n",
    "        self.combinedPixels = data[3]\n",
    "        self.numEpochs = int((np.size(data)-3)/3)+1 #int((np.size(data) - 3) / 3) \n",
    "      #  self.field = self.data[3].header['SOURCEF'][19:21] \n",
    "        self.cdelt1 = 0 #self.combinedFlux.header['cdelt1']  # Wavelength interval between subsequent pixels \n",
    "        self.crpix1 = 0 # self.combinedFlux.header['crpix1']\n",
    "        self.crval1 = 0 #self.combinedFlux.header['crval1']\n",
    "        self.n_pix = self.combinedFlux.header['NAXIS1']\n",
    "        self.RA = self.combinedFlux.header['RA']\n",
    "        self.DEC = self.combinedFlux.header['DEC']\n",
    "        self.wavelength = data[0].data\n",
    "\n",
    "        self.fluxCoadd = self.combinedFlux.data\n",
    "        self.varianceCoadd = self.combinedVariance.data\n",
    "        self.badpixCoadd = self.combinedPixels.data\n",
    "\n",
    "        self._wavelength = None\n",
    "        self._flux = None\n",
    "        self._variance = None\n",
    "        self._badpix = None\n",
    "        self._dates = None\n",
    "        self._run = None\n",
    "        self._ext = None\n",
    "        self._qc = None\n",
    "        self._exposed = None\n",
    "\n",
    "\n",
    "    @property\n",
    "    def flux(self):\n",
    "        if getattr(self, '_flux', None) is None:\n",
    "            self._flux = np.zeros((len(self.data[0].data), self.numEpochs), dtype=float) #2848 or 5000\n",
    "            for i in range(self.numEpochs):\n",
    "                self._flux[:, i] = self.data[i*3+1].data*10**17   \n",
    "        return self._flux\n",
    "\n",
    "    @property\n",
    "    def variance(self):\n",
    "        if getattr(self, '_variance', None) is None:\n",
    "            self._variance = np.zeros((len(self.data[0].data), self.numEpochs), dtype=float)\n",
    "            for i in range(self.numEpochs):\n",
    "                self._variance[:, i] = self.data[i*3 + 2].data*10**34\n",
    "        return self._variance\n",
    "\n",
    "    @property\n",
    "    def badpix(self):\n",
    "        if getattr(self, '_badpix', None) is None:\n",
    "            self._badpix = np.zeros((len(self.data[0].data), self.numEpochs), dtype=float)\n",
    "            for i in range(self.numEpochs):\n",
    "                self._badpix[:, i] = self.data[i * 3 + 3].data\n",
    "        return self._badpix\n",
    "\n",
    "    @property\n",
    "    def dates(self):\n",
    "        if getattr(self, '_dates', None) is None:\n",
    "            self._dates = np.zeros(self.numEpochs, dtype=float)\n",
    "            for i in range(self.numEpochs):\n",
    "                self._dates[i] = round(self.data[i * 2 + 1].header['DATE-OBS'],3) +i\n",
    "        return self._dates\n",
    "\n",
    "\n",
    "    @property\n",
    "    def ext(self):\n",
    "        if getattr(self, '_ext', None) is None:\n",
    "            self._ext = []\n",
    "            for i in range(self.numEpochs):\n",
    "                self._ext.append(i * 2)  # gives the extension in original fits file\n",
    "        return self._ext\n",
    "\n",
    "    @property\n",
    "    def run(self):\n",
    "        if getattr(self, '_run', None) is None:\n",
    "            self._run = []\n",
    "            for i in range(self.numEpochs):\n",
    "                self._run.append(1)  # this gives the run number of the observation\n",
    "\n",
    "        return self._run\n",
    "\n",
    "    @property\n",
    "    def qc(self):\n",
    "        if getattr(self, '_qc', None) is None:\n",
    "            self._qc = []\n",
    "            for i in range(self.numEpochs):\n",
    "                self._qc.append('ok')\n",
    "                # this tell you if there were any problems with the spectra that need to be masked out\n",
    "        return self._qc\n",
    "\n",
    "    @property\n",
    "    def exposed(self):\n",
    "        if getattr(self, '_exposed', None) is None:\n",
    "            self._exposed = []\n",
    "            for i in range(self.numEpochs):\n",
    "                self._exposed.append(self.data[i * 2+1].header['EXPTIME']) \n",
    "                # this will give you the exposure time of each observation\n",
    "        return self._exposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------- #\n",
    "# ------------------- calibSpec -------------------- #\n",
    "# -------------------------------------------------- #\n",
    "# This function does the bulk of the work.  It will  #\n",
    "# 1) determine extensions which can be calibrated    #\n",
    "# 2) calculate the scale factors                     #\n",
    "# 3) calculate the warping function                  #\n",
    "# 4) output new fits file with scaled spectra        #\n",
    "# -------------------------------------------------- #\n",
    "\n",
    "def calibSpec(obj_name, spectra, photo, spectraName, photoName, outBase, bands, filters, centers, plotFlag, coaddFlag,\n",
    "              interpFlag, redshift):\n",
    "    # Assumes scaling given is of the form\n",
    "    # gScale = scaling[0,:]   gError = scaling[3,:]\n",
    "    # rScale = scaling[1,:]   rError = scaling[4,:]\n",
    "    # iScale = scaling[2,:]   iError = scaling[5,:]\n",
    "    # inCoaddWeather = scaling[6,:]\n",
    "    # inCoaddPhoto = scaling[7,:]\n",
    "    # gMag = scaling[8,:]   gMagErr = scaling[9,:]\n",
    "    # rMag = scaling[10,:]  rMagErr = scaling[11,:]\n",
    "    # iMag = scaling[12,:]  iMagErr = scaling[13,:]\n",
    "\n",
    "    # First we decide which extensions are worth scaling\n",
    "    print('executing calibSpec')\n",
    "    \n",
    "    extensions, noPhotometry, badQC = prevent_Excess(spectra, photo, bands, interpFlag)\n",
    "    # Then we calculate the scale factors\n",
    "    if plotFlag != False:\n",
    "        plotName = plotFlag + obj_name\n",
    "    else:\n",
    "        plotName = False\n",
    "    nevermind, scaling = scaling_Matrix(spectra, extensions, badQC, noPhotometry, photo, bands, filters, interpFlag,\n",
    "                                        plotName)\n",
    "    # Remove last minute trouble makers\n",
    "    extensions = [e for e in extensions if e not in nevermind]\n",
    "    badQC = badQC + nevermind\n",
    "    \n",
    "    \n",
    "    # And finally warp the data\n",
    "    for s in extensions:\n",
    "        # scale the spectra\n",
    "        if plotFlag != False:\n",
    "            plotName = plotFlag + obj_name + \"_\" + str(s)\n",
    "        else:\n",
    "            plotName = False\n",
    "        spectra.flux[:, s], spectra.variance[:, s] = warp_spectra(scaling[0:3, s], scaling[3:6, s], spectra.flux[:, s],\n",
    "                                                                  spectra.variance[:, s], spectra.wavelength, centers,\n",
    "                                                                  plotName, bands, filters)\n",
    "\n",
    "    if coaddFlag == False:\n",
    "        create_output_single(obj_name, extensions, scaling, spectra, noPhotometry, badQC, spectraName, photoName,\n",
    "                             outBase, redshift)\n",
    "    elif coaddFlag in ['Run', 'Date']:\n",
    "        coadd_output(obj_name, extensions, scaling, spectra, noPhotometry, badQC, spectraName, photoName, outBase,\n",
    "                     plotFlag, coaddFlag, redshift)\n",
    "    else:\n",
    "        print(\"What do you want me to do with this data? Please specify output type.\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------- #\n",
    "# ---------------- prevent_Excess ------------------ #\n",
    "# -------------------------------------------------- #\n",
    "# This function removes extensions from the list to  #\n",
    "# calibrate because of insufficient photometric data #\n",
    "# or bad quality flags                               #\n",
    "# -------------------------------------------------- #\n",
    "\n",
    "def prevent_Excess(spectra, photo, bands, interpFlag):\n",
    "    # First, find the min/max date for which we have photometry taken on each side of the spectroscopic observation\n",
    "    # This will be done by finding the highest date for which we have photometry in each band\n",
    "    # and taking the max/min of those values\n",
    "    # This is done because we perform a linear interpolation between photometric data points to estimate the magnitudes\n",
    "    # observed at the specific time of the spectroscopic observation\n",
    "    # If you want to use the Gaussian process fitting you can forecast into the future/past by the number of days\n",
    "    # set by the delay term.\n",
    "\n",
    "    maxPhot = np.zeros(3)\n",
    "\n",
    "    # If using Gaussian process fitting you can forecast up to 28 days.  You probably want to make some plots to check\n",
    "    # this isn't crazy though!\n",
    "    delay = 0\n",
    "    if interpFlag == 'BBK':\n",
    "        delay = 28\n",
    "\n",
    "    for e in range(len(photo['Date'][:])):\n",
    "        if photo['Band'][e] == bands[0]:\n",
    "            if photo['Date'][e] > maxPhot[0]:\n",
    "                maxPhot[0] = photo['Date'][e]\n",
    "        if photo['Band'][e] == bands[1]:\n",
    "            if photo['Date'][e] > maxPhot[1]:\n",
    "                maxPhot[1] = photo['Date'][e]\n",
    "        if photo['Band'][e] == bands[2]:\n",
    "            if photo['Date'][e] > maxPhot[2]:\n",
    "                maxPhot[2] = photo['Date'][e]\n",
    "    photLim = min(maxPhot) + delay\n",
    "\n",
    "\n",
    "    minPhot = np.array([100000, 100000, 100000])\n",
    "    for e in range(len(photo['Date'][:])):\n",
    "        if photo['Band'][e] == bands[0]:\n",
    "            if photo['Date'][e] < minPhot[0]:\n",
    "                minPhot[0] = photo['Date'][e]\n",
    "        if photo['Band'][e] == bands[1]:\n",
    "            if photo['Date'][e] < minPhot[1]:\n",
    "                minPhot[1] = photo['Date'][e]\n",
    "        if photo['Band'][e] == bands[2]:\n",
    "            if photo['Date'][e] < minPhot[2]:\n",
    "                minPhot[2] = photo['Date'][e]\n",
    "    photLimMin = max(minPhot) - delay\n",
    "\n",
    "    \n",
    "    noPhotometry = []\n",
    "    badQC = []\n",
    "\n",
    "    allowedQC = ['ok', 'backup']\n",
    "\n",
    "    for s in range(spectra.numEpochs):\n",
    "        # Remove data with insufficient photometry\n",
    "        if spectra.dates[s] > photLim:\n",
    "            noPhotometry.append(s)\n",
    "        if spectra.dates[s] < photLimMin:\n",
    "            noPhotometry.append(s)\n",
    "        # Only allow spectra with quality flags 'ok' and 'backup'\n",
    "        if spectra.qc[s] not in allowedQC:\n",
    "\n",
    "            badQC.append(s)\n",
    "\n",
    "    extensions = []\n",
    "    \n",
    "    # Make a list of extensions which need to be analyzed\n",
    "    for s in range(spectra.numEpochs):\n",
    "        if s not in noPhotometry and s not in badQC:\n",
    "            extensions.append(s)\n",
    "\n",
    "    return extensions, noPhotometry, badQC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------- #\n",
    "# ---------------- scaling_Matrix ------------------ #\n",
    "# -------------------------------------------------- #\n",
    "# finds the nearest photometry and interpolates mags #\n",
    "# to find values at the time of the spectroscopic    #\n",
    "# observations.  Calculates the mag that would be    #\n",
    "# observed from the spectra and calculates the scale #\n",
    "# factor to bring them into agreement. Saves the     #\n",
    "# data in the scaling matrix.                        #\n",
    "# -------------------------------------------------- #\n",
    "\n",
    "def scaling_Matrix(spectra, extensions, badQC, noPhotometry, photo, bands, filters, interpFlag, plotFlag):\n",
    "    # scale factors for each extension saved in the following form\n",
    "    # gScale = scaling[0,:]   gError = scaling[3,:]\n",
    "    # rScale = scaling[1,:]   rError = scaling[4,:]\n",
    "    # iScale = scaling[2,:]   iError = scaling[5,:]\n",
    "    # inCoaddWeather = scaling[6,:]\n",
    "    # inCoaddPhoto = scaling[7,:]\n",
    "    # gMag = scaling[8,:]   gMagError = scaling[9,:] (interpolated from neighbouring observations)\n",
    "    # rMag = scaling[10,:]   rMagError = scaling[11,:]\n",
    "    # iMag = scaling[12,:]   iMagError = scaling[13,:]\n",
    "\n",
    "    scaling = np.zeros((14, spectra.numEpochs))\n",
    "\n",
    "    # Judge goodness of spectra\n",
    "    for e in range(spectra.numEpochs):\n",
    "        if e in badQC:\n",
    "            scaling[6, e] = False\n",
    "        else:\n",
    "            scaling[6, e] = True\n",
    "        if e in noPhotometry:\n",
    "            scaling[7, e] = False\n",
    "        else:\n",
    "            scaling[7, e] = True\n",
    "\n",
    "    ozdesPhoto = np.zeros((3, spectra.numEpochs))\n",
    "    desPhoto = np.zeros((3, spectra.numEpochs))\n",
    "\n",
    "    ozdesPhotoU = np.zeros((3, spectra.numEpochs))\n",
    "    desPhotoU = np.zeros((3, spectra.numEpochs))\n",
    "\n",
    "    filterCurves = readFilterCurves(bands, filters)\n",
    "\n",
    "    if interpFlag == 'BBK':\n",
    "        desPhoto, desPhotoU = des_photo_BBK(photo, spectra.dates, bands, spectra.numEpochs, plotFlag)\n",
    "\n",
    "        scaling[8, :] = desPhoto[0, :]\n",
    "        scaling[10, :] = desPhoto[1, :]\n",
    "        scaling[12, :] = desPhoto[2, :]\n",
    "\n",
    "        scaling[9, :] = desPhotoU[0, :]\n",
    "        scaling[11, :] = desPhotoU[1, :]\n",
    "        scaling[13, :] = desPhotoU[2, :]\n",
    "\n",
    "    nevermind = []\n",
    "\n",
    "    for e in extensions:\n",
    "        # Find OzDES photometry\n",
    "\n",
    "        ozdesPhoto[0, e], ozdesPhotoU[0, e] = computeABmag(filterCurves[bands[0]].trans, filterCurves[bands[0]].wave,\n",
    "                                                           spectra.wavelength, spectra.flux[:, e],\n",
    "                                                           spectra.variance[:, e])\n",
    "        ozdesPhoto[1, e], ozdesPhotoU[1, e] = computeABmag(filterCurves[bands[1]].trans, filterCurves[bands[1]].wave,\n",
    "                                                           spectra.wavelength, spectra.flux[:, e],\n",
    "                                                           spectra.variance[:, e])\n",
    "        ozdesPhoto[2, e], ozdesPhotoU[2, e] = computeABmag(filterCurves[bands[2]].trans, filterCurves[bands[2]].wave,\n",
    "                                                           spectra.wavelength, spectra.flux[:, e],\n",
    "                                                           spectra.variance[:, e])\n",
    "\n",
    "\n",
    "        # Sometimes the total flux in the band goes zero and this obviously creates issues further down the line and\n",
    "        # is most noticeable when the calculated magnitude is nan.  Sometimes it is because the data is very noisy\n",
    "        # or the occasional negative spectrum is a known artifact of the data, more common in early OzDES runs.  In the\n",
    "        # case where the observation doesn't get cut based on quality flag it will start getting ignored here.  The runs\n",
    "        # ignored will eventually be saved with the badQC extensions.\n",
    "\n",
    "        if np.isnan(ozdesPhoto[:, e]).any() == True:\n",
    "            nevermind.append(e)\n",
    "\n",
    "        # Find DES photometry\n",
    "        if interpFlag == 'linear':\n",
    "            desPhoto[:, e], desPhotoU[:, e] = des_photo(photo, spectra.dates[e], bands)\n",
    "\n",
    "            scaling[8, e] = desPhoto[0, e]\n",
    "            scaling[10, e] = desPhoto[1, e]\n",
    "            scaling[12, e] = desPhoto[2, e]\n",
    "\n",
    "            scaling[9, e] = desPhotoU[0, e]\n",
    "            scaling[11, e] = desPhotoU[1, e]\n",
    "            scaling[13, e] = desPhotoU[2, e]\n",
    "\n",
    "        # Find Scale Factor\n",
    "        scaling[0, e], scaling[3, e] = scale_factors(desPhoto[0, e] - ozdesPhoto[0, e],\n",
    "                                                     desPhotoU[0, e] + ozdesPhotoU[0, e])\n",
    "        scaling[1, e], scaling[4, e] = scale_factors(desPhoto[1, e] - ozdesPhoto[1, e],\n",
    "                                                     desPhotoU[1, e] + ozdesPhotoU[1, e])\n",
    "        scaling[2, e], scaling[5, e] = scale_factors(desPhoto[2, e] - ozdesPhoto[2, e],\n",
    "                                                     desPhotoU[2, e] + ozdesPhotoU[2, e])\n",
    "\n",
    "\n",
    "\n",
    "    return nevermind, scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------- #\n",
    "# The next three functions are modified from code    #\n",
    "# provided by Dale Mudd                              #\n",
    "# -------------------------------------------------- #\n",
    "# ------------------ filterCurve ------------------- #\n",
    "# -------------------------------------------------- #\n",
    "# creates a class to hold the transmission function  #\n",
    "# for each band.                                     #\n",
    "# -------------------------------------------------- #\n",
    "\n",
    "class filterCurve:\n",
    "    \"\"\"A filter\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.wave = np.array([], 'float')\n",
    "        self.trans = np.array([], 'float')\n",
    "        return\n",
    "\n",
    "    def read(self, file):\n",
    "        # DES filter curves express the wavelengths in nms\n",
    "        if 'DES' in file:\n",
    "            factor = 10.\n",
    "        else:\n",
    "            factor = 1.\n",
    "        file = open(file, 'r')\n",
    "        for line in file.readlines():\n",
    "            if line[0] != '#':\n",
    "                entries = line.split()\n",
    "                self.wave = np.append(self.wave, float(entries[0]))\n",
    "                self.trans = np.append(self.trans, float(entries[1]))\n",
    "        file.close()\n",
    "        # We use Angstroms for the wavelength in the filter transmission file\n",
    "        self.wave = self.wave * factor\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------- #\n",
    "# ---------------- readFilterCurve ----------------- #\n",
    "# -------------------------------------------------- #\n",
    "# Reads in the filter curves and stores it as the    #\n",
    "# filter curve class.                                #\n",
    "# -------------------------------------------------- #\n",
    "\n",
    "def readFilterCurves(bands, filters):\n",
    "\n",
    "    filterCurves = {}\n",
    "    for f in bands:\n",
    "        filterCurves[f] = filterCurve()\n",
    "        filterCurves[f].read(filters[f])\n",
    "\n",
    "    return filterCurves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------- #\n",
    "# ----------------- computeABmag ------------------- #\n",
    "# -------------------------------------------------- #\n",
    "# computes the AB magnitude for given transmission   #\n",
    "# functions and spectrum (f_lambda).  Returns the    #\n",
    "# magnitude and variance.                            #\n",
    "# -------------------------------------------------- #\n",
    "\n",
    "def computeABmag(trans_flux, trans_wave, tmp_wave, tmp_flux, tmp_var):\n",
    "    # Takes and returns variance\n",
    "    # trans_ : transmission function data\n",
    "    # tmp_ : spectral data\n",
    "\n",
    "    # trans/tmp not necessarily defined over the same wavelength range\n",
    "    # first determine the wavelength range over which both are defined\n",
    "    minV = min(trans_wave)\n",
    "    if minV < min(tmp_wave):\n",
    "        minV = min(tmp_wave)\n",
    "    maxV = max(trans_wave)\n",
    "    if maxV > max(trans_wave):\n",
    "        maxV = max(trans_wave)\n",
    "\n",
    "    interp_wave = []\n",
    "    tmp_flux2 = []\n",
    "    tmp_var2 = []\n",
    "\n",
    "    # Make new vectors for the flux just using that range (assuming spectral binning)\n",
    "\n",
    "    for i in range(len(tmp_wave)):\n",
    "        if minV < tmp_wave[i] < maxV:\n",
    "            interp_wave.append(tmp_wave[i])\n",
    "            tmp_flux2.append(tmp_flux[i])\n",
    "            tmp_var2.append(tmp_var[i])\n",
    "\n",
    "    # interpolate the transmission function onto this range\n",
    "    # the transmission function is interpolated as it is generally much smoother than the spectral data\n",
    "    trans_flux2 = interp1d(trans_wave, trans_flux)(interp_wave)\n",
    "\n",
    "    # And now calculate the magnitude and uncertainty\n",
    "\n",
    "    c = 2.992792e18  # Angstrom/s\n",
    "    Num = np.nansum(tmp_flux2 * trans_flux2 * interp_wave)\n",
    "    Num_var = np.nansum(tmp_var2 * (trans_flux2 * interp_wave) ** 2)\n",
    "    Den = np.nansum(trans_flux2 / interp_wave)\n",
    "\n",
    "    with np.errstate(divide='raise'):\n",
    "        try:\n",
    "            magAB = -2.5 * np.log10(Num / Den / c) - 48.60\n",
    "            magABvar = 1.17882 * Num_var / (Num ** 2)\n",
    "        except FloatingPointError:\n",
    "            magAB = 99.\n",
    "            magABvar = 99.\n",
    "\n",
    "    return magAB, magABvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------- #\n",
    "# ------------------ des_photo  -------------------- #\n",
    "# -------------------------------------------------- #\n",
    "# Finds nearest photometry on both sides of spectral #\n",
    "# observations and interpolates to find value at the #\n",
    "# time of the spectral observation.                  #\n",
    "# -------------------------------------------------- #\n",
    "\n",
    "def des_photo(photo, spectral_mjd, bands):\n",
    "\n",
    "    \"\"\"Takes in an mjd from the spectra, looks through a light curve file to find the nearest photometric epochs and\n",
    "    performs linear interpolation to get estimate at date, return the photo mags.   \"\"\"\n",
    "\n",
    "    # Assumes dates are in chronological order!!!\n",
    "    mags = np.zeros(3)\n",
    "    errs = np.zeros(3)\n",
    "    len(photo['Date'])\n",
    "    for l in range(len(photo['Date'])-1):#-1 ):\n",
    "\n",
    "        if photo['Band'][l] == bands[0] and photo['Date'][l] < spectral_mjd < photo['Date'][l + 1]:\n",
    "            g_date_v = np.array([photo['Date'][l], photo['Date'][l + 1]])\n",
    "            g_mag_v = np.array([photo['Mag'][l], photo['Mag'][l + 1]])\n",
    "            g_err_v = np.array([photo['Mag_err'][l], photo['Mag_err'][l + 1]])\n",
    "        if photo['Band'][l] == bands[1] and photo['Date'][l] < spectral_mjd < photo['Date'][l + 1]:\n",
    "            r_date_v = np.array([photo['Date'][l], photo['Date'][l + 1]])\n",
    "            r_mag_v = np.array([photo['Mag'][l], photo['Mag'][l + 1]])\n",
    "            r_err_v = np.array([photo['Mag_err'][l], photo['Mag_err'][l + 1]])\n",
    "        if photo['Band'][l] == bands[2] and photo['Date'][l] < spectral_mjd < photo['Date'][l + 1]:\n",
    "            i_date_v = np.array([photo['Date'][l], photo['Date'][l + 1]])\n",
    "            i_mag_v = np.array([photo['Mag'][l], photo['Mag'][l + 1]])\n",
    "            i_err_v = np.array([photo['Mag_err'][l], photo['Mag_err'][l + 1]])\n",
    "\n",
    "    mags[0], errs[0] = interpolatePhot(g_date_v, g_mag_v, g_err_v, spectral_mjd)\n",
    "    mags[1], errs[1] = interpolatePhot(r_date_v, r_mag_v, r_err_v, spectral_mjd)\n",
    "    mags[2], errs[2] = interpolatePhot(i_date_v, i_mag_v, i_err_v, spectral_mjd)\n",
    "\n",
    "\n",
    "    return mags, errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------- #\n",
    "# ---------------- des_photo_BBK  ------------------ #\n",
    "# -------------------------------------------------- #\n",
    "# Finds nearest photometry on both sides of spectral #\n",
    "# observations and interpolates to find value at the #\n",
    "# time of the spectral observations using Brownian   #\n",
    "# Bridge Gaussian processes.  This is better for     #\n",
    "# sparser data.                                      #\n",
    "# -------------------------------------------------- #\n",
    "\n",
    "def des_photo_BBK(photo, dates, bands, numEpochs, plotFlag):\n",
    "\n",
    "    # Assumes dates are in chronological order!!!\n",
    "    mags = np.zeros((3, numEpochs))\n",
    "\n",
    "    errs = np.zeros((3, numEpochs))\n",
    "\n",
    "    # Fit a Brownian Bridge Kernel to the data via Gaussian processes.\n",
    "    for b in range(3):\n",
    "        x = []  # Dates for each band\n",
    "        y = []  # Mags for each band\n",
    "        s = []  # Errors for each band\n",
    "\n",
    "        # get data for each band\n",
    "        for l in range(len(photo['Date']) - 1):\n",
    "            if photo['Band'][l] == bands[b]:\n",
    "                x.append(photo['Date'][l])\n",
    "                y.append(photo['Mag'][l])\n",
    "                s.append(photo['Mag_err'][l])\n",
    "\n",
    "        x = np.array(x)\n",
    "        y = np.array(y)\n",
    "        s = np.array(s)\n",
    "\n",
    "        # Define kernel for Gaussian process: Browning Bridge x Constant\n",
    "        kernel1 = BBK(length_scale=25, length_scale_bounds=(1, 1000))\n",
    "        kernel2 = kernels.ConstantKernel(constant_value=1.0, constant_value_bounds=(0.001, 10.0))\n",
    "        gp = GaussianProcessRegressor(kernel=kernel1 * kernel2, alpha=s ** 2, normalize_y=True)\n",
    "\n",
    "        # Fit the data with the model\n",
    "        xprime = np.atleast_2d(x).T\n",
    "        yprime = np.atleast_2d(y).T\n",
    "        gp.fit(xprime, yprime)\n",
    "\n",
    "        if plotFlag != False:\n",
    "            # Plot what the model looks like\n",
    "            bname = ['_g', '_r', '_i']\n",
    "            preddates = np.linspace(min(x) - 100, max(x) + 100, 3000)\n",
    "            y_predAll, sigmaAll = gp.predict(np.atleast_2d(preddates).T, return_std=True)\n",
    "            y_predAll = y_predAll.flatten()\n",
    "            fig, ax1 = makeFigSingle(plotFlag + bname[b], 'Date', 'Mag', [dates[0], dates[-1]])\n",
    "\n",
    "            # I want to plot lines where the observations take place - only plot one per night though\n",
    "            dateCull = dates.astype(int)\n",
    "            dateCull = np.unique(dateCull)\n",
    "            for e in range(len(dateCull)):\n",
    "                ax1.axvline(dateCull[e], color='grey', alpha=0.5)\n",
    "            ax1.errorbar(x, y, yerr=s, fmt='o', color='mediumblue', markersize='7')\n",
    "\n",
    "            # Plot model with error bars.\n",
    "            ax1.plot(preddates, y_predAll, color='black')\n",
    "            ax1.fill_between(preddates, y_predAll - sigmaAll, y_predAll + sigmaAll, alpha=0.5, color='black')\n",
    "            plt.savefig(plotFlag + bname[b] + \"_photoModel.png\")\n",
    "            plt.close(fig)\n",
    "\n",
    "        # Predict photometry vales for each observation\n",
    "        y_pred, sigma = gp.predict(np.atleast_2d(dates).T, return_std=True)\n",
    "        mags[b, :] = y_pred.flatten()\n",
    "        errs[b, :] = sigma[0]**2\n",
    "\n",
    "    return mags, errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------- #\n",
    "# --------------- interpolatePhot  ----------------- #\n",
    "# -------------------------------------------------- #\n",
    "# Performs linear interpolation and propagates the   #\n",
    "# uncertainty to return you a variance.              #\n",
    "# -------------------------------------------------- #\n",
    "\n",
    "def interpolatePhot(x, y, s, val):\n",
    "    # takes sigma returns variance\n",
    "    # x - x data points (list)\n",
    "    # y - y data points (list)\n",
    "    # s - sigma on y data points (list)\n",
    "    # val - x value to interpolate to (number)\n",
    "\n",
    "    mag = y[0] + (val - x[0]) * (y[1] - y[0]) / (x[1] - x[0])\n",
    "\n",
    "    err = s[0] ** 2 + (s[0] ** 2 + s[1] ** 2) * ((val - x[0]) / (x[1] - x[0])) ** 2\n",
    "\n",
    "    return mag, err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------- #\n",
    "# ---------------- scale_factors  ------------------ #\n",
    "# -------------------------------------------------- #\n",
    "# Calculates the scale factor and variance needed to #\n",
    "# change spectroscopically derived magnitude to the  #\n",
    "# observed photometry.                               #\n",
    "# -------------------------------------------------- #\n",
    "\n",
    "def scale_factors(mag_diff, mag_diff_var):\n",
    "    # takes and returns variance\n",
    "\n",
    "    flux_ratio = np.power(10., 0.4 * mag_diff)  # f_synthetic/f_photometry\n",
    "    scale_factor = (1. / flux_ratio)\n",
    "    scale_factor_sigma = mag_diff_var * (scale_factor * 0.4 * 2.3) ** 2   # ln(10) ~ 2.3\n",
    "\n",
    "    return scale_factor, scale_factor_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------- #\n",
    "# ----------------- warp_spectra  ------------------ #\n",
    "# -------------------------------------------------- #\n",
    "# Fits polynomial to scale factors and estimates     #\n",
    "# associated uncertainties with gaussian processes.  #\n",
    "# If the plotFlag variable is not False it will save #\n",
    "# some diagnostic plots.                             #\n",
    "# -------------------------------------------------- #\n",
    "\n",
    "def warp_spectra(scaling, scaleErr, flux, variance, wavelength, centers, plotFlag, bands, filters):\n",
    "\n",
    "    # associate scale factors with centers of bands and fit 2D polynomial to form scale function.\n",
    "#    scale = UnivariateSpline(centers, scaling, k=1)\n",
    "    scale = InterpolatedUnivariateSpline(centers, scaling, k=2)\n",
    "\n",
    "    fluxScale = flux * scale(wavelength)\n",
    "    # add in Gaussian process to estimate uncertainties, /10**-17 because it gets a bit panicky if you use small numbers\n",
    "    stddev = (scaleErr ** 0.5) / 10 ** -17\n",
    "    scale_v = scaling / 10 ** -17\n",
    "\n",
    "    kernel = kernels.RBF(length_scale=300, length_scale_bounds=(.01, 2000.0)) #300 and 2000\n",
    "\n",
    "    gp = GaussianProcessRegressor(kernel=kernel, alpha=stddev**2)\n",
    "\n",
    "    xprime = np.atleast_2d(centers).T\n",
    "    yprime = np.atleast_2d(scale_v).T\n",
    "    \n",
    "    gp.fit(xprime, yprime)\n",
    "    xplot_prime = np.atleast_2d(wavelength).T\n",
    "    y_pred, sigma = gp.predict(xplot_prime, return_std=True)\n",
    "\n",
    "    y_pred = y_pred[:,0]\n",
    "    sigModel = (sigma/y_pred)*scale(wavelength)\n",
    "    # now scale the original variance and combine with scale factor uncertainty\n",
    "    varScale = variance * pow(scale(wavelength), 2) + sigModel ** 2\n",
    "\n",
    "    if plotFlag != False:\n",
    "        figa, ax1a, ax2a = makeFigDouble(plotFlag, \"Wavelength ($\\AA$)\", \"f$_\\lambda$ (arbitrary units)\",\n",
    "                                      \"f$_\\lambda$ (10$^{-17}$ erg/s/cm$^2$/$\\AA$)\", [wavelength[0], wavelength[-1]])\n",
    "\n",
    "        ax1a.plot(wavelength, flux, color='black', label=\"Before Calibration\")\n",
    "        ax1a.legend(loc=1, frameon=False, prop={'size': 20})\n",
    "        ax2a.plot(wavelength, fluxScale / 10 ** -17, color='black', label=\"After Calibration\")\n",
    "        ax2a.legend(loc=1, frameon=False, prop={'size': 20})\n",
    "        plt.savefig(plotFlag + \"_beforeAfter.png\")\n",
    "        plt.close(figa)\n",
    "\n",
    "        figb, ax1b, ax2b = makeFigDouble(plotFlag, \"Wavelength ($\\AA$)\", \"f$_\\lambda$ (10$^{-17}$ erg/s/cm$^2$/$\\AA$)\",\n",
    "                                         \"% Uncertainty\", [wavelength[0], wavelength[-1]])\n",
    "        ax1b.plot(wavelength, fluxScale / 10 ** -17, color='black')\n",
    "\n",
    "        ax2b.plot(wavelength, 100*abs(pow(varScale, 0.5)/fluxScale), color='black', linestyle='-', label='Total')\n",
    "        ax2b.plot(wavelength, 100*abs(sigModel/fluxScale), color='blue', linestyle='-.', label='Warping')\n",
    "        ax2b.legend(loc=1, frameon=False, prop={'size': 20})\n",
    "        ax2b.set_ylim([0, 50])\n",
    "        plt.savefig(plotFlag + \"_uncertainty.png\")\n",
    "        plt.close(figb)\n",
    "\n",
    "        figc, axc = makeFigSingle(plotFlag, \"Wavelength ($\\AA$)\", \"Scale Factor (10$^{-17}$ erg/s/cm$^2$/$\\AA$/counts)\")\n",
    "        axc.plot(wavelength, scale(wavelength)/10**-17, color='black')\n",
    "        axc.errorbar(centers, scaling/10**-17, yerr=stddev, fmt='s', color='mediumblue')\n",
    "        plt.savefig(plotFlag + \"_scalefactors.png\")\n",
    "        plt.close(figc)\n",
    "        \n",
    "        figd, ax1d, ax2d, ax3d, ax4d = makeFigQuadruple(plotFlag, \"Wavelength ($\\AA$)\",\"Uncalibrated f$_\\lambda$\", \"Transmission\",\n",
    "                                                        \"Scale Factors\", \"Calibrated f$_\\lambda$ \", \n",
    "                                                       [wavelength[0], wavelength[-1]])\n",
    "        ax1d.plot(wavelength, flux, color='black')\n",
    "        ax4d.plot(wavelength, fluxScale / 10 ** -17, color='black')\n",
    "        ax3d.plot(wavelength, scale(wavelength)/10**-17, color = 'black')\n",
    "        ax3d.errorbar(centers[0], scaling[0]/10**-17, yerr=stddev[0], fmt='s', color='g', marker = 'o')\n",
    "        ax3d.errorbar(centers[1], scaling[1]/10**-17, yerr=stddev[1], fmt='s', color='r', marker = 'v')\n",
    "        ax3d.errorbar(centers[2], scaling[2]/10**-17, yerr=stddev[2], fmt='s', color='blue', marker = 's')\n",
    "        filterCurves = readFilterCurves(bands, filters)\n",
    "        ax2d.plot(filterCurves[bands[0]].wave, filterCurves[bands[0]].trans, color = 'g')\n",
    "        ax2d.text(centers[0], 0.3, 'g', fontdict={'color': 'g', 'size': 18})\n",
    "        ax2d.plot(filterCurves[bands[1]].wave, filterCurves[bands[1]].trans, color = 'r', linestyle='dashed')\n",
    "        ax2d.text(centers[1], 0.3, 'r', fontdict={'color': 'r', 'size': 18})\n",
    "        ax2d.plot(filterCurves[bands[2]].wave, filterCurves[bands[2]].trans, color = 'b', linestyle='dotted')\n",
    "        ax2d.text(centers[2], 0.3, 'i', fontdict={'color': 'b', 'size': 18})\n",
    "        plt.savefig(plotFlag + \"_combinedfigure.png\")\n",
    "        plt.close(figd)\n",
    "\n",
    "    return fluxScale, varScale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------- #\n",
    "# ------------ create_output_single  --------------- #\n",
    "# -------------------------------------------------- #\n",
    "# Outputs the warped spectra to a new fits file.     #\n",
    "# -------------------------------------------------- #\n",
    "def create_output_single(obj_name, extensions, scaling, spectra, noPhotometry, badQC, spectraName, photoName, outBase,\n",
    "                         redshift):\n",
    "\n",
    "    outName = outBase + obj_name + \"_scaled.fits\"\n",
    "    print(\"Saving Data to \" + outName)\n",
    "\n",
    "    hdulist = fits.HDUList(fits.PrimaryHDU())\n",
    "\n",
    "    noPhotometryExt = []\n",
    "    if len(noPhotometry) > 0:\n",
    "        for i in range(len(noPhotometry)):\n",
    "            noPhotometryExt.append(spectra.ext[noPhotometry[i]])\n",
    "\n",
    "    badQCExt = []\n",
    "    if len(badQC) > 0:\n",
    "        for i in range(len(badQC)):\n",
    "            badQCExt.append(spectra.ext[badQC[i]])\n",
    "\n",
    "    index = 0\n",
    "    # Create an HDU for each night\n",
    "    for i in extensions:\n",
    "        header = fits.Header()\n",
    "        header['SOURCE'] = obj_name\n",
    "        header['RA'] = spectra.RA\n",
    "        header['DEC'] = spectra.DEC\n",
    "        header['CRPIX1'] = spectra.crpix1\n",
    "        header['CRVAL1'] = spectra.crval1\n",
    "        header['CDELT1'] = spectra.cdelt1\n",
    "        header['CTYPE1'] = 'wavelength'\n",
    "        header['CUNIT1'] = 'angstrom'\n",
    "        header['EPOCHS'] = len(extensions)\n",
    "        header['z'] = redshift[0]\n",
    "\n",
    "        # save the names of the input data and the extensions ignored\n",
    "        header['SFILE'] = spectraName\n",
    "        header['PFILE'] = photoName\n",
    "        header['NOPHOTO'] = ','.join(map(str, noPhotometryExt))\n",
    "        header['BADQC'] = ','.join(map(str, badQCExt))\n",
    "\n",
    "        # save the original spectrum's extension number and some other details\n",
    "        header[\"EXT\"] = spectra.ext[i]\n",
    "        header[\"UTMJD\"] = spectra.dates[i]\n",
    "        header[\"EXPOSE\"] = spectra.exposed[i]\n",
    "        header[\"QC\"] = spectra.qc[i]\n",
    "\n",
    "        # save scale factors/uncertainties\n",
    "        header[\"SCALEG\"] = scaling[0, i]\n",
    "        header[\"ERRORG\"] = scaling[3, i]\n",
    "        header[\"SCALER\"] = scaling[1, i]\n",
    "        header[\"ERRORR\"] = scaling[4, i]\n",
    "        header[\"SCALEI\"] = scaling[2, i]\n",
    "        header[\"ERRORI\"] = scaling[5, i]\n",
    "\n",
    "        # save photometry/uncertainties used to calculate scale factors\n",
    "        header[\"MAGG\"] = scaling[8, i]\n",
    "        header[\"MAGUG\"] = scaling[9, i]\n",
    "        header[\"MAGR\"] = scaling[10, i]\n",
    "        header[\"MAGUR\"] = scaling[11, i]\n",
    "        header[\"MAGI\"] = scaling[12, i]\n",
    "        header[\"MAGUI\"] = scaling[13, i]\n",
    "        if index == 0:\n",
    "            hdulist[0].header['SOURCE'] = obj_name\n",
    "            hdulist[0].header['RA'] = spectra.RA\n",
    "            hdulist[0].header['DEC'] = spectra.DEC\n",
    "            hdulist[0].header['CRPIX1'] = spectra.crpix1\n",
    "            hdulist[0].header['CRVAL1'] = spectra.crval1\n",
    "            hdulist[0].header['CDELT1'] = spectra.cdelt1\n",
    "            hdulist[0].header['CTYPE1'] = 'wavelength'\n",
    "            hdulist[0].header['CUNIT1'] = 'angstrom'\n",
    "            hdulist[0].header['EPOCHS'] = len(extensions)\n",
    "\n",
    "            # save the names of the input data and the extensions ignored\n",
    "            hdulist[0].header['SFILE'] = spectraName\n",
    "            hdulist[0].header['PFILE'] = photoName\n",
    "            hdulist[0].header['NOPHOTO'] = ','.join(map(str, noPhotometryExt))\n",
    "            hdulist[0].header['BADQC'] = ','.join(map(str, badQCExt))\n",
    "\n",
    "            # save the original spectrum's extension number and some other details\n",
    "            hdulist[0].header[\"EXT\"] = spectra.ext[i]\n",
    "            hdulist[0].header[\"UTMJD\"] = spectra.dates[i]\n",
    "            hdulist[0].header[\"EXPOSE\"] = spectra.exposed[i]\n",
    "            hdulist[0].header[\"QC\"] = spectra.qc[i]\n",
    "\n",
    "            # save scale factors/uncertainties\n",
    "            hdulist[0].header[\"SCALEG\"] = scaling[0, i]\n",
    "            hdulist[0].header[\"ERRORG\"] = scaling[3, i]\n",
    "            hdulist[0].header[\"SCALER\"] = scaling[1, i]\n",
    "            hdulist[0].header[\"ERRORR\"] = scaling[4, i]\n",
    "            hdulist[0].header[\"SCALEI\"] = scaling[2, i]\n",
    "            hdulist[0].header[\"ERRORI\"] = scaling[5, i]\n",
    "\n",
    "            # save photometry/uncertainties used to calculate scale factors\n",
    "            hdulist[0].header[\"MAGG\"] = scaling[8, i]\n",
    "            hdulist[0].header[\"MAGUG\"] = scaling[9, i]\n",
    "            hdulist[0].header[\"MAGR\"] = scaling[10, i]\n",
    "            hdulist[0].header[\"MAGUR\"] = scaling[11, i]\n",
    "            hdulist[0].header[\"MAGI\"] = scaling[12, i]\n",
    "            hdulist[0].header[\"MAGUI\"] = scaling[13, i]\n",
    "            hdulist[0].header[\"UTMJD\"] = spectra.dates[i]\n",
    "            hdulist[0].data = spectra.wavelength\n",
    "            hdulist.append(fits.ImageHDU(data=spectra.flux[:, i], header=header))\n",
    "            hdulist.append(fits.ImageHDU(data=spectra.variance[:, i], header=header))\n",
    "            hdulist.append(fits.ImageHDU(data=spectra.badpix[:, i], header=header))\n",
    "            index = 2\n",
    "            \n",
    "\n",
    "        else:\n",
    "            hdulist.append(fits.ImageHDU(data=spectra.flux[:, i], header=header))\n",
    "            hdulist.append(fits.ImageHDU(data=spectra.variance[:, i], header=header))\n",
    "            hdulist.append(fits.ImageHDU(data=spectra.badpix[:, i], header=header))\n",
    "    hdulist.writeto(outName, overwrite=True)\n",
    "    hdulist.close()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------- #\n",
    "# ------------- create_output_coadd  --------------- #\n",
    "# -------------------------------------------------- #\n",
    "# Outputs the warped and coadded spectra to a new    #\n",
    "# fits file.                                         #\n",
    "# -------------------------------------------------- #\n",
    "\n",
    "\n",
    "def create_output_coadd(obj_name, runList, fluxArray, varianceArray, badpixArray, extensions, scaling, spectra, redshift\n",
    "                        ,badQC, noPhotometry, spectraName, photoName, outBase, coaddFlag):\n",
    "\n",
    "    outName = outBase + obj_name + \"_scaled_\" + coaddFlag + \".fits\"\n",
    "    hdulist = fits.HDUList(fits.PrimaryHDU())\n",
    "\n",
    "    noPhotometryExt = []\n",
    "    if len(noPhotometry) > 0:\n",
    "        for i in range(len(noPhotometry)):\n",
    "            noPhotometryExt.append(spectra.ext[noPhotometry[i]])\n",
    "\n",
    "    badQCExt = []\n",
    "    if len(badQC) > 0:\n",
    "        for i in range(len(badQC)):\n",
    "            badQCExt.append(spectra.ext[badQC[i]])\n",
    "\n",
    "    print(\"Output Filename: %s \\n\" % (outName))\n",
    "    # First save the total coadded spectrum for the source to the primary extension\n",
    "    hdulist[0].data= spectra.wavelength\n",
    "    hdulist[0].header['CRPIX1'] = spectra.crpix1\n",
    "    hdulist[0].header['CRVAL1'] = spectra.crval1\n",
    "    hdulist[0].header['CDELT1'] = spectra.cdelt1\n",
    "    hdulist[0].header['CTYPE1'] = 'wavelength'\n",
    "    hdulist[0].header['CUNIT1'] = 'angstrom'\n",
    "    hdulist[0].header['SOURCE'] = obj_name\n",
    "    hdulist[0].header['RA'] = spectra.RA\n",
    "    hdulist[0].header['DEC'] = spectra.DEC\n",
    "    hdulist[0].header['OBSNUM'] = len(runList)\n",
    "    hdulist[0].header['z'] = redshift[0]\n",
    "    hdulist[0].header['SFILE'] = spectraName\n",
    "    hdulist[0].header['PFILE'] = photoName\n",
    "    hdulist[0].header['METHOD'] = coaddFlag\n",
    "    hdulist[0].header['NOPHOTO'] = ','.join(map(str, noPhotometryExt))\n",
    "    hdulist[0].header['BADQC'] = ','.join(map(str, badQCExt))\n",
    "\n",
    "    ################# First extension is the flux\n",
    "    header = fits.Header()\n",
    "    header['EXTNAME'] = 'FLUX'\n",
    "    header['CRPIX1'] = spectra.crpix1\n",
    "    header['CRVAL1'] = spectra.crval1\n",
    "    header['CDELT1'] = spectra.cdelt1\n",
    "    header['CTYPE1'] = 'wavelength'\n",
    "    header['CUNIT1'] = 'angstrom'\n",
    "    hdulist.append(fits.ImageHDU(data=fluxArray[:, 0], header=header))\n",
    "    \n",
    "    # First extension is the total coadded variance\n",
    "    header = fits.Header()\n",
    "    header['EXTNAME'] = 'VARIANCE'\n",
    "    header['CRPIX1'] = spectra.crpix1\n",
    "    header['CRVAL1'] = spectra.crval1\n",
    "    header['CDELT1'] = spectra.cdelt1\n",
    "    header['CTYPE1'] = 'wavelength'\n",
    "    header['CUNIT1'] = 'angstrom'\n",
    "    hdulist.append(fits.ImageHDU(data=varianceArray[:, 0], header=header))\n",
    "\n",
    "    # Second Extension is the total bad pixel map\n",
    "    header = fits.Header()\n",
    "    header['EXTNAME'] = 'BadPix'\n",
    "    header['CRPIX1'] = spectra.crpix1\n",
    "    header['CRVAL1'] = spectra.crval1\n",
    "    header['CDELT1'] = spectra.cdelt1\n",
    "    header['CTYPE1'] = 'wavelength'\n",
    "    header['CUNIT1'] = 'angstrom'\n",
    "    hdulist.append(fits.ImageHDU(data=badpixArray[:, 0], header=header))\n",
    "\n",
    "    # Create an HDU for each night\n",
    "    index1 = 1\n",
    "    for k in runList:\n",
    "        print('creating HDU')\n",
    "        index = 0\n",
    "        date = 0\n",
    "        header = fits.Header()\n",
    "        header['CRPIX1'] = spectra.crpix1\n",
    "        header['CRVAL1'] = spectra.crval1\n",
    "        header['CDELT1'] = spectra.cdelt1\n",
    "        header['CTYPE1'] = 'wavelength'\n",
    "        header['CUNIT1'] = 'angstrom'\n",
    "        header['RUN'] = k\n",
    "        for i in extensions:\n",
    "            here = False\n",
    "            if coaddFlag == 'Run':\n",
    "                if spectra.run[i] == k:\n",
    "                    here = True\n",
    "\n",
    "            if coaddFlag == 'Date':\n",
    "                if int(spectra.dates[i]) == k:\n",
    "                    here = True\n",
    "\n",
    "            if here == True:\n",
    "                head0 = \"EXT\" + str(index)\n",
    "                header[head0] = spectra.ext[i]\n",
    "\n",
    "                head1 = \"UTMJD\" + str(index)\n",
    "                header[head1] = spectra.dates[i]\n",
    "                date += spectra.dates[i]\n",
    "\n",
    "                head2 = \"EXPOSE\" + str(index)\n",
    "                header[head2] = spectra.exposed[i]\n",
    "\n",
    "                head3 = \"QC\" + str(index)\n",
    "                header[head3] = spectra.qc[i]\n",
    "\n",
    "                head4 = \"SCALEG\" + str(index)\n",
    "                header[head4] = scaling[0, i]\n",
    "\n",
    "                head5 = \"ERRORG\" + str(index)\n",
    "                header[head5] = scaling[3, i]\n",
    "\n",
    "                head6 = \"SCALER\" + str(index)\n",
    "                header[head6] = scaling[1, i]\n",
    "\n",
    "                head7 = \"ERRORR\" + str(index)\n",
    "                header[head7] = scaling[4, i]\n",
    "\n",
    "                head8 = \"SCALEI\" + str(index)\n",
    "                header[head8] = scaling[2, i]\n",
    "\n",
    "                head9 = \"ERRORI\" + str(index)\n",
    "                header[head9] = scaling[5, i]\n",
    "\n",
    "                head10 = \"MAGG\" + str(index)\n",
    "                header[head10] = scaling[8, i]\n",
    "\n",
    "                head11 = \"MAGUG\" + str(index)\n",
    "                header[head11] = scaling[9, i]\n",
    "\n",
    "                head12 = \"MAGR\" + str(index)\n",
    "                header[head12] = scaling[10, i]\n",
    "\n",
    "                head13 = \"MAGUR\" + str(index)\n",
    "                header[head13] = scaling[11, i]\n",
    "\n",
    "                head14 = \"MAGI\" + str(index)\n",
    "                header[head14] = scaling[12, i]\n",
    "\n",
    "                head15 = \"MAGUI\" + str(index)\n",
    "                header[head15] = scaling[13, i]\n",
    "\n",
    "                index += 1\n",
    "\n",
    "        if date > 0:\n",
    "            header['OBSNUM'] = index\n",
    "            header['AVGDATE'] = date / index\n",
    "\n",
    "            hdu_flux = fits.ImageHDU(data=fluxArray[:, index1], header=header)\n",
    "            hdu_fluxvar = fits.ImageHDU(data=varianceArray[:, index1], header=header)\n",
    "            hdu_badpix = fits.ImageHDU(data=badpixArray[:, index1], header=header)\n",
    "            hdulist.append(hdu_flux)\n",
    "            hdulist.append(hdu_fluxvar)\n",
    "            hdulist.append(hdu_badpix)\n",
    "        index1 += 1\n",
    "\n",
    "    hdulist.writeto(outName, overwrite=True)\n",
    "    hdulist.close()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------- #\n",
    "# ----------------- coadd_output  ------------------ #\n",
    "# -------------------------------------------------- #\n",
    "# Coadds the observations based on run or night.     #\n",
    "# -------------------------------------------------- #\n",
    "def coadd_output(obj_name, extensions, scaling, spectra, noPhotometry, badQC, spectraName, photoName, outBase, plotFlag,\n",
    "                 coaddFlag, redshift):\n",
    "\n",
    "    # Get a list of items (dates/runs) over which all observations will be coadded\n",
    "    coaddOver = []\n",
    "\n",
    "    for e in extensions:\n",
    "        # OzDES runs 7,8 were close together in time and run 8 had bad weather so there was only observations of 1\n",
    "        # field - coadd with run 7 to get better signal to noise\n",
    "        #if spectra.run[e] == 8:\n",
    "        #    spectra.run[e] = 7\n",
    "\n",
    "        if coaddFlag == 'Run':\n",
    "            if spectra.run[e] not in coaddOver:\n",
    "                coaddOver.append(spectra.run[e])\n",
    "\n",
    "        if coaddFlag == 'Date':\n",
    "            if int(spectra.dates[e]) not in coaddOver:\n",
    "                coaddOver.append(int(spectra.dates[e]))\n",
    "\n",
    "\n",
    "    coaddFlux = np.zeros((len(spectra.flux), len(coaddOver) + 1))\n",
    "    coaddVar = np.zeros((len(spectra.flux), len(coaddOver) + 1))\n",
    "    coaddBadPix = np.zeros((len(spectra.flux), len(coaddOver) + 1))\n",
    "\n",
    "    speclistC = []  # For total coadd of observation\n",
    "    index = 1\n",
    "\n",
    "    for c in coaddOver:\n",
    "        speclist = []\n",
    "        for e in extensions:\n",
    "            opt = ''\n",
    "            if coaddFlag == 'Run':\n",
    "                opt = spectra.run[e]\n",
    "            if coaddFlag == 'Date':\n",
    "                opt = int(spectra.dates[e])\n",
    "            if opt == c:\n",
    "                speclist.append(SingleSpec(obj_name, spectra.wavelength, spectra.flux[:,e], spectra.variance[:,e],\n",
    "                                           spectra.badpix[:,e]))\n",
    "                speclistC.append(SingleSpec(obj_name, spectra.wavelength, spectra.flux[:,e], spectra.variance[:,e],\n",
    "                                            spectra.badpix[:,e]))\n",
    "\n",
    "        if len(speclist) > 1:\n",
    "            runCoadd = outlier_reject_and_coadd(obj_name, speclist)\n",
    "            coaddFlux[:, index] = runCoadd.flux\n",
    "            coaddVar[:, index] = runCoadd.fluxvar\n",
    "            coaddVar[:, index] = runCoadd.fluxvar\n",
    "            coaddBadPix[:,index] = runCoadd.isbad.astype('uint8')\n",
    "        if len(speclist) == 1:\n",
    "            coaddFlux[:, index] = speclist[0].flux\n",
    "            coaddVar[:, index] = speclist[0].fluxvar\n",
    "            coaddBadPix[:, index] = speclist[0].isbad.astype('uint8')\n",
    "        index += 1\n",
    "\n",
    "    if len(speclistC) > 1:\n",
    "        allCoadd = outlier_reject_and_coadd(obj_name, speclistC)\n",
    "        coaddFlux[:, 0] = allCoadd.flux\n",
    "        coaddVar[:, 0] = allCoadd.fluxvar\n",
    "        coaddBadPix[:, 0] = allCoadd.isbad.astype('uint8')\n",
    "    if len(speclistC) == 1:\n",
    "        coaddFlux[:, 0] = speclistC[0].flux\n",
    "        coaddVar[:, 0] = speclistC[0].fluxvar\n",
    "        coaddBadPix[:, 0] = speclistC[0].isbad.astype('uint8')\n",
    "\n",
    "    mark_as_bad(coaddFlux, coaddVar)\n",
    "\n",
    "    create_output_coadd(obj_name, coaddOver, coaddFlux, coaddVar, coaddBadPix, extensions, scaling, spectra, redshift,\n",
    "                        badQC, noPhotometry, spectraName, photoName, outBase, coaddFlag)\n",
    "\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------- #\n",
    "# Modified from code originally provided by          #\n",
    "# Harry Hobson                                       #\n",
    "# -------------------------------------------------- #\n",
    "# ------------------ mark_as_bad ------------------- #\n",
    "# -------------------------------------------------- #\n",
    "# Occasionally you get some big spikes in the data   #\n",
    "# that you do not want messing with your magnitude   #\n",
    "# calculations.  Remove these by looking at single   #\n",
    "# bins that have a significantly 4.5 larger than     #\n",
    "# average fluxes or variances and change those to    #\n",
    "# nans. Nans will be interpolated over.  The         #\n",
    "# threshold should be chosen to weigh removing       #\n",
    "# extreme outliers and removing noise.               #\n",
    "# -------------------------------------------------- #\n",
    "\n",
    "def mark_as_bad(fluxes, variances):\n",
    "    number = int(fluxes.size/fluxes.shape[0])\n",
    "    for epoch in range(number):\n",
    "        if number == 1:\n",
    "            flux = fluxes[:]\n",
    "            variance = variances[:]\n",
    "        else:\n",
    "            flux = fluxes[:, epoch]\n",
    "            variance = variances[:, epoch]\n",
    "\n",
    "        nBins = len(flux)\n",
    "        # define the local average in flux and variance to compare outliers to\n",
    "        for i in range(nBins):\n",
    "            if i < 50:\n",
    "                avg = np.nanmean(variance[0:99])\n",
    "                avgf = np.nanmean(flux[0:99])\n",
    "            elif i > nBins - 50:\n",
    "                avg = np.nanmean(variance[i-50:nBins-1])\n",
    "                avgf = np.nanmean(flux[i-50:nBins-1])\n",
    "            else:\n",
    "                avg = np.nanmean(variance[i-50:i+50])\n",
    "                avgf = np.nanmean(flux[i-50:i+50])\n",
    "\n",
    "            # find outliers and set that bin and the neighbouring ones to nan.\n",
    "\n",
    "            if np.isnan(variance[i]) == False and variance[i] > 4.5*avg:\n",
    "\n",
    "                flux[i] = np.nan\n",
    "                if i > 2 and i < 4996:\n",
    "                    flux[i - 1] = np.nan\n",
    "                    flux[i - 2] = np.nan\n",
    "                    flux[i - 3] = np.nan\n",
    "                    flux[i + 1] = np.nan\n",
    "                    flux[i + 2] = np.nan\n",
    "                    flux[i + 3] = np.nan\n",
    "\n",
    "            if np.isnan(flux[i]) == False and flux[i] > 4.5 * avgf:\n",
    "\n",
    "                flux[i] = np.nan\n",
    "                if i > 2 and i < 4996:\n",
    "                    flux[i-1] = np.nan\n",
    "                    flux[i-2] = np.nan\n",
    "                    flux[i-3] = np.nan\n",
    "                    flux[i+1] = np.nan\n",
    "                    flux[i+2] = np.nan\n",
    "                    flux[i+3] = np.nan\n",
    "\n",
    "            if np.isnan(flux[i]) == False and flux[i] < -4.5 * avgf:\n",
    "\n",
    "                flux[i] = np.nan\n",
    "                if i > 2 and i < 4996:\n",
    "                    flux[i-1] = np.nan\n",
    "                    flux[i-2] = np.nan\n",
    "                    flux[i-3] = np.nan\n",
    "                    flux[i+1] = np.nan\n",
    "                    flux[i+2] = np.nan\n",
    "                    flux[i+3] = np.nan\n",
    "\n",
    "        # interpolates nans (added here and bad pixels in the data)\n",
    "        filter_bad_pixels(flux, variance)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------- #\n",
    "# Modified from code originally provided by          #\n",
    "# Harry Hobson                                       #\n",
    "# -------------------------------------------------- #\n",
    "# --------------- filter_bad_pixels ---------------- #\n",
    "# -------------------------------------------------- #\n",
    "# Interpolates over nans in the spectrum.            #\n",
    "# -------------------------------------------------- #\n",
    "def filter_bad_pixels(fluxes, variances):\n",
    "    number = int(fluxes.size/fluxes.shape[0])\n",
    "    for epoch in range(number):\n",
    "        if (number == 1):\n",
    "            flux = fluxes[:]\n",
    "            variance = variances[:]\n",
    "        else:\n",
    "            flux = fluxes[:, epoch]\n",
    "            variance = variances[:, epoch]\n",
    "\n",
    "        nBins = len(flux)\n",
    "\n",
    "        flux[0] = np.nanmean(flux)/1000\n",
    "        flux[-1] = np.nanmean(flux)/1000\n",
    "        variance[0] = 100*np.nanmean(variance)\n",
    "        variance[-1] = 100*np.nanmean(variance)\n",
    "\n",
    "        bad_pixels = np.logical_or.reduce((np.isnan(flux), np.isnan(variance), variance < 0))\n",
    "\n",
    "        bin = 0\n",
    "        binEnd = 0\n",
    "\n",
    "        while (bin < nBins):\n",
    "            if (bad_pixels[bin] == True):\n",
    "                binStart = bin\n",
    "                binNext = bin + 1\n",
    "                while (binNext < nBins):\n",
    "                    if bad_pixels[binNext] == False:\n",
    "                        binEnd = binNext - 1\n",
    "                        binNext = nBins\n",
    "                    binNext = binNext + 1\n",
    "\n",
    "                ya = float(flux[binStart - 1])\n",
    "                xa = float(binStart - 1)\n",
    "                sa = variance[binStart - 1]\n",
    "                yb = flux[binEnd + 1]\n",
    "                xb = binEnd + 1\n",
    "                sb = variance[binEnd + 1]\n",
    "\n",
    "                step = binStart\n",
    "                while (step < binEnd + 1):\n",
    "                    flux[step] = ya + (yb - ya) * (step - xa) / (xb - xa)\n",
    "                    variance[step] = sa + (sb + sa) * ((step - xa) / (xb - xa)) ** 2\n",
    "                    step = step + 1\n",
    "                bin = binEnd\n",
    "            bin = bin + 1\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------- #\n",
    "# ----------------- makeFigDouble ------------------ #\n",
    "# -------------------------------------------------- #\n",
    "# -------------------------------------------------- #\n",
    "# A function that defines a figure and axes with two #\n",
    "# panels that shares an x axis and has legible axis  #\n",
    "# labels.                                            #\n",
    "# -------------------------------------------------- #\n",
    "font = {'size': '20', 'color': 'black', 'weight': 'normal'}\n",
    "\n",
    "def makeFigDouble(title, xlabel, ylabel1, ylabel2, xlim=[0, 0], ylim1=[0, 0], ylim2=[0, 0]):\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, sharex=True)\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(10, 10, forward=True)\n",
    "    fig.subplots_adjust(hspace=0)\n",
    "\n",
    "    for label in (ax1.get_xticklabels() + ax1.get_yticklabels()):\n",
    "        label.set_fontsize(20)\n",
    "    for label in (ax2.get_xticklabels() + ax2.get_yticklabels()):\n",
    "        label.set_fontsize(20)\n",
    "\n",
    "    ax1.set_ylabel(ylabel1, **font)\n",
    "    if ylim1 != [0, 0] and ylim1[0] < ylim1[1]:\n",
    "        ax1.set_ylim(ylim1)\n",
    "\n",
    "    ax2.set_ylabel(ylabel2, **font)\n",
    "    if ylim2 != [0, 0] and ylim2[0] < ylim2[1]:\n",
    "        ax2.set_ylim(ylim2)\n",
    "\n",
    "    ax2.set_xlabel(xlabel, **font)\n",
    "    if xlim != [0, 0] and xlim[0] < xlim[1]:\n",
    "        ax2.set_xlim(xlim)\n",
    "\n",
    "    ax1.set_title(title, **font)\n",
    "\n",
    "    return fig, ax1, ax2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------- #\n",
    "# ----------------- makeFigSingle ------------------ #\n",
    "# -------------------------------------------------- #\n",
    "# -------------------------------------------------- #\n",
    "# A function that defines a figure with legible axis #\n",
    "# labels.                                            #\n",
    "# -------------------------------------------------- #\n",
    "def makeFigSingle(title, xlabel, ylabel, xlim=[0, 0], ylim=[0, 0]):\n",
    "    fig = plt.figure()\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(10, 10, forward=True)\n",
    "\n",
    "    ax = fig.add_subplot(111)\n",
    "    for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        label.set_fontsize(20)\n",
    "\n",
    "    ax.set_ylabel(ylabel, **font)\n",
    "    if ylim != [0, 0] and ylim[0] < ylim[1]:\n",
    "        ax.set_ylim(ylim)\n",
    "\n",
    "    ax.set_xlabel(xlabel, **font)\n",
    "    if xlim != [0, 0] and xlim[0] < xlim[1]:\n",
    "        ax.set_xlim(xlim)\n",
    "\n",
    "    ax.set_title(title, **font)\n",
    "\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------- #\n",
    "# ----------------- makeFigQuadruple ------------------ #\n",
    "# -------------------------------------------------- #\n",
    "# -------------------------------------------------- #\n",
    "# A function that defines a figure with legible axis #\n",
    "# labels.                                            #\n",
    "# -------------------------------------------------- #\n",
    "\n",
    "def makeFigQuadruple(title, xlabel, ylabel1, ylabel2, ylabel3, ylabel4, xlim=[0, 0], ylim1=[0, 0], ylim2=[0, 0], ylim3=[0, 0], ylim4=[0,0]):\n",
    "    fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, sharex=True)\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(10, 16, forward=True)\n",
    "    fig.subplots_adjust(hspace=0)\n",
    "\n",
    "    for label in (ax1.get_xticklabels() + ax1.get_yticklabels()):\n",
    "        label.set_fontsize(20)\n",
    "    for label in (ax2.get_xticklabels() + ax2.get_yticklabels()):\n",
    "        label.set_fontsize(20)\n",
    "    for label in (ax3.get_xticklabels() + ax3.get_yticklabels()):\n",
    "        label.set_fontsize(20)\n",
    "    for label in (ax4.get_xticklabels() + ax4.get_yticklabels()):\n",
    "        label.set_fontsize(20)\n",
    "\n",
    "    ax1.set_ylabel(ylabel1, **axis_font)\n",
    "    if ylim1 != [0, 0] and ylim1[0] < ylim1[1]:\n",
    "        ax1.set_ylim(ylim1)\n",
    "\n",
    "    ax2.set_ylabel(ylabel2, **axis_font)\n",
    "    if ylim2 != [0, 0] and ylim2[0] < ylim2[1]:\n",
    "        ax2.set_ylim(ylim2)\n",
    "\n",
    "    ax3.set_ylabel(ylabel3, **axis_font)\n",
    "    if ylim3 != [0, 0] and ylim3[0] < ylim3[1]:\n",
    "        ax3.set_ylim(ylim3)\n",
    "        \n",
    "    ax4.set_ylabel(ylabel4, **axis_font)\n",
    "    if ylim4 != [0, 0] and ylim4[0] < ylim4[1]:\n",
    "        ax4.set_ylim(ylim4)\n",
    "\n",
    "    ax4.set_xlabel(xlabel, **axis_font)\n",
    "    if xlim != [0, 0] and xlim[0] < xlim[1]:\n",
    "        ax4.set_xlim(xlim)\n",
    "\n",
    "    ax1.set_title(title, **title_font)\n",
    "\n",
    "    return fig, ax1, ax2, ax3, ax4\n",
    "\n",
    "title_font = {'size':'22', 'color':'black', 'weight':'normal', 'verticalalignment':'bottom'}\n",
    "axis_font = {'size':'22'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------- #\n",
    "#  The following 4 functions were written by Chris   #\n",
    "# Lidman, Mike Childress, and maybe others for the   #\n",
    "# initial processing of the OzDES spectra.  They     #\n",
    "# were taken from the DES_coaddSpectra.py functions. #\n",
    "# -------------------------------------------------- #\n",
    "# -------------------- OzExcept -------------------- #\n",
    "# -------------------------------------------------- #\n",
    "# -------------------------------------------------- #\n",
    "# A simple exception class                           #\n",
    "# -------------------------------------------------- #\n",
    "\n",
    "\n",
    "class OzExcept(Exception):\n",
    "    \"\"\"\n",
    "    Simple exception class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, msg):\n",
    "        self.msg = msg\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"{0}: {1}\".format(self.__class__.__name__, msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------- #\n",
    "# ----------------- VerboseMessager ---------------- #\n",
    "# -------------------------------------------------- #\n",
    "# -------------------------------------------------- #\n",
    "# Verbose messaging for routines below.              #\n",
    "# -------------------------------------------------- #\n",
    "\n",
    "\n",
    "class VerboseMessager(object):\n",
    "    \"\"\"\n",
    "    Verbose messaging for routines below\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, verbose=False):\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __call__(self, *args):\n",
    "        if self.verbose:\n",
    "            print(\"Something strange is happening\")\n",
    "            sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------- #\n",
    "# ------------------- SingleSpec ------------------- #\n",
    "# -------------------------------------------------- #\n",
    "# -------------------------------------------------- #\n",
    "# Class representing a single spectrum for analysis. #\n",
    "# -------------------------------------------------- #\n",
    "class SingleSpec(object):\n",
    "    \"\"\"\n",
    "    Class representing a single spectrum for analysis\n",
    "    \"\"\"\n",
    "\n",
    "    ## Added filename to SingleSpec\n",
    "    def __init__(self, obj_name, wl, flux, fluxvar, badpix):\n",
    "\n",
    "        self.name = obj_name\n",
    "        # ---------------------------\n",
    "        # self.pivot = int(fibrow[9])\n",
    "        # self.xplate = int(fibrow[3])\n",
    "        # self.yplate = int(fibrow[4])\n",
    "        # self.ra = np.degrees(fibrow[1])\n",
    "        # self.dec = np.degrees(fibrow[2])\n",
    "        # self.mag=float(fibrow[10])\n",
    "        # self.header=header\n",
    "\n",
    "        self.wl = np.array(wl)\n",
    "        self.flux = np.array(flux)\n",
    "        self.fluxvar = np.array(fluxvar)\n",
    "\n",
    "        # If there is a nan in either the flux, or the variance, mark it as bad\n",
    "\n",
    "        # JKH: this was what was here originally, my version complains about it\n",
    "        # self.fluxvar[fluxvar < 0] = np.nan\n",
    "\n",
    "        for i in range(len(spectra.flux)):\n",
    "            if (self.fluxvar[i] < 0):\n",
    "                self.fluxvar[i] = np.nan\n",
    "\n",
    "        # The following doesn't take into account\n",
    "        #self.isbad = np.any([np.isnan(self.flux), np.isnan(self.fluxvar)], axis=0)\n",
    "        self.isbad = badpix.astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------- #\n",
    "# ------------ outlier_reject_and_coadd ------------ #\n",
    "# -------------------------------------------------- #\n",
    "# -------------------------------------------------- #\n",
    "# OzDES coadding function to reject outliers and     #\n",
    "# coadd all of the spectra in the inputted list.     #\n",
    "# -------------------------------------------------- #\n",
    "def outlier_reject_and_coadd(obj_name, speclist):\n",
    "    \"\"\"\n",
    "    Reject outliers on single-object spectra to be coadded.\n",
    "    Assumes input spectra have been resampled to a common wavelength grid,\n",
    "    so this step needs to be done after joining and resampling.\n",
    "\n",
    "    Inputs\n",
    "        speclist:  list of SingleSpec instances on a common wavelength grid\n",
    "        show:  boolean; show diagnostic plot?  (debug only; default=False)\n",
    "        savefig:  boolean; save diagnostic plot?  (debug only; default=False)\n",
    "    Output\n",
    "        result:  SingleSpec instance of coadded spectrum, with bad pixels\n",
    "            set to np.nan (runz requires this)\n",
    "    \"\"\"\n",
    "\n",
    "    # Edge cases\n",
    "    if len(speclist) == 0:\n",
    "        print(\"outlier_reject:  empty spectrum list\")\n",
    "        return None\n",
    "    elif len(speclist) == 1:\n",
    "        tgname = speclist[0].name\n",
    "        vmsg(\"Only one spectrum, no coadd needed for {0}\".format(tgname))\n",
    "        return speclist[0]\n",
    "\n",
    "    # Have at least two spectra, so let's try to reject outliers\n",
    "    # At this stage, all spectra have been mapped to a common wavelength scale\n",
    "    wl = speclist[0].wl\n",
    "    tgname = speclist[0].name\n",
    "    # Retrieve single-object spectra and variance spectra.\n",
    "    flux_2d = np.array([s.flux for s in speclist])\n",
    "    fluxvar_2d = np.array([s.fluxvar for s in speclist])\n",
    "    badpix_2d = np.array([s.isbad for s in speclist])\n",
    "\n",
    "\n",
    "    # Baseline parameters:\n",
    "    #    outsig     Significance threshold for outliers (in sigma)\n",
    "    #    nbin       Bin width for median rebinning\n",
    "    #    ncoinc     Maximum number of spectra in which an artifact can appear\n",
    "    outsig, nbin, ncoinc = 5, 25, 1\n",
    "    nspec, nwl = flux_2d.shape\n",
    "\n",
    "    # Run a median filter of the spectra to look for n-sigma outliers.\n",
    "    # These incantations are kind of complicated but they seem to work\n",
    "    # i) Compute the median of a wavelength section (nbin) along the observation direction\n",
    "    # 0,1 : observation,wavelength, row index, column index\n",
    "    # In moving to numpy v1.10.2, we replaced median with nanmedian\n",
    "    fmed = np.reshape([np.nanmedian(flux_2d[:, j:j + nbin], axis=1)\n",
    "                       for j in np.arange(0, nwl, nbin)], (-1, nspec)).T\n",
    "\n",
    "    # Now expand fmed and flag pixels that are more than outsig off\n",
    "    fmed_2d = np.reshape([fmed[:, int(j / nbin)] for j in np.arange(nwl)], (-1, nspec)).T\n",
    "\n",
    "    resid = (flux_2d - fmed_2d) / np.sqrt(fluxvar_2d)\n",
    "    # If the residual is nan, set flag_2d to 1\n",
    "    nans = np.isnan(resid)\n",
    "\n",
    "    flag_2d = np.zeros(nspec * nwl).reshape(nspec, nwl)\n",
    "    flag_2d[nans] = 1\n",
    "    flag_2d[~nans] = (np.abs(resid[~nans]) > outsig)\n",
    "\n",
    "    # If a pixel is flagged in only one spectrum, it's probably a cosmic ray\n",
    "    # and we should mark it as bad and add ito to badpix_2d.  Otherwise, keep it.\n",
    "    # This may fail if we coadd many spectra and a cosmic appears in 2 pixels\n",
    "    # For these cases, we could increase ncoinc\n",
    "    flagsum = np.tile(np.sum(flag_2d, axis=0), (nspec, 1))\n",
    "    # flag_2d, flagsum forms a tuple of 2 2d arrays\n",
    "    # If flag_2d is true and if and flagsum <= ncoinc then set that pixel to bad.\n",
    "    badpix_2d[np.all([flag_2d, flagsum <= ncoinc], axis=0)] = True\n",
    "\n",
    "\n",
    "    # Remove bad pixels in the collection of spectra.  In the output they\n",
    "    # must appear as NaN, but any wavelength bin which is NaN in one spectrum\n",
    "    # will be NaN in the coadd.  So we need to set the bad pixel values to\n",
    "    # something innocuous like the median flux, then set the weights of the\n",
    "    # bad pixels to zero in the coadd.  If a wavelength bin is bad in all\n",
    "    # the coadds, it's just bad and needs to be marked as NaN in the coadd.\n",
    "    # In moving to numpy v1.10.2, we replaced median with nanmedian\n",
    "    flux_2d[badpix_2d] = np.nanmedian(fluxvar_2d)\n",
    "    fluxvar_2d[badpix_2d] = np.nanmedian(fluxvar_2d)\n",
    "    badpix_coadd = np.all(badpix_2d, axis=0)\n",
    "    # Derive the weights\n",
    "    ## Use just the variance\n",
    "    wi = 1.0 / (fluxvar_2d)\n",
    "    # Set the weights of bad data to zero\n",
    "    wi[badpix_2d] = 0.0\n",
    "    # Why set the weight of the just first spectrum to np.nan?\n",
    "    # If just one of the mixels is nan, then the result computed below is nan as well\n",
    "    for i, val in enumerate(badpix_coadd):\n",
    "        if val:  wi[0, i] = np.nan\n",
    "\n",
    "    # Some coadd\n",
    "    coaddflux = np.average(flux_2d, weights=wi, axis=0)\n",
    "    coaddfluxvar = np.average(fluxvar_2d, weights=wi, axis=0) / nspec\n",
    "\n",
    "    coaddflux[badpix_coadd] = np.nan\n",
    "    coaddfluxvar[badpix_coadd] = np.nan\n",
    "\n",
    "    # Return the coadded spectrum in a SingleSpectrum object\n",
    "    return SingleSpec(obj_name, wl, coaddflux, coaddfluxvar, badpix_coadd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------- #\n",
    "# ----------------------- BBK ---------------------- #\n",
    "# -------------------------------------------------- #\n",
    "# -------------------------------------------------- #\n",
    "# A Brownian Bridge Kernel to use with sklearn       #\n",
    "# Gaussian Processes to interpolate between          #\n",
    "# photometry.  I have really just copied             #\n",
    "# Scikit-learn's RBF kernel and modified it to be a  #\n",
    "# brownian bridge (sqeuclidian -> euclidian).        #\n",
    "# -------------------------------------------------- #\n",
    "class BBK(kernels.StationaryKernelMixin, kernels.NormalizedKernelMixin, kernels.Kernel):\n",
    "    # Here I am slightly modifying scikit-learn's RBF Kernel to do\n",
    "    # the brownian bridge.\n",
    "\n",
    "    \"\"\"Radial-basis function kernel (aka squared-exponential kernel).\n",
    "    The RBF kernel is a stationary kernel. It is also known as the\n",
    "    \"squared exponential\" kernel. It is parameterized by a length-scale\n",
    "    parameter length_scale>0, which can either be a scalar (isotropic variant\n",
    "    of the kernel) or a vector with the same number of dimensions as the inputs\n",
    "    X (anisotropic variant of the kernel). The kernel is given by:\n",
    "    k(x_i, x_j) = exp(-1 / 2 d(x_i / length_scale, x_j / length_scale)^2)\n",
    "    This kernel is infinitely differentiable, which implies that GPs with this\n",
    "    kernel as covariance function have mean square derivatives of all orders,\n",
    "    and are thus very smooth.\n",
    "    .. versionadded:: 0.18\n",
    "    Parameters\n",
    "    ----------\n",
    "    length_scale : float or array with shape (n_features,), default: 1.0\n",
    "        The length scale of the kernel. If a float, an isotropic kernel is\n",
    "        used. If an array, an anisotropic kernel is used where each dimension\n",
    "        of l defines the length-scale of the respective feature dimension.\n",
    "    length_scale_bounds : pair of floats >= 0, default: (1e-5, 1e5)\n",
    "        The lower and upper bound on length_scale\n",
    "    \"\"\"\n",
    "    def __init__(self, length_scale=1.0, length_scale_bounds=(1e-5, 1e5)):\n",
    "        self.length_scale = length_scale\n",
    "        self.length_scale_bounds = length_scale_bounds\n",
    "\n",
    "    @property\n",
    "    def anisotropic(self):\n",
    "        return np.iterable(self.length_scale) and len(self.length_scale) > 1\n",
    "\n",
    "    @property\n",
    "    def hyperparameter_length_scale(self):\n",
    "        if self.anisotropic:\n",
    "            return kernels.Hyperparameter(\"length_scale\", \"numeric\",\n",
    "                                  self.length_scale_bounds,\n",
    "                                  len(self.length_scale))\n",
    "        return kernels.Hyperparameter(\n",
    "            \"length_scale\", \"numeric\", self.length_scale_bounds)\n",
    "\n",
    "    def __call__(self, X, Y=None, eval_gradient=False):\n",
    "        \"\"\"Return the kernel k(X, Y) and optionally its gradient.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array, shape (n_samples_X, n_features)\n",
    "            Left argument of the returned kernel k(X, Y)\n",
    "        Y : array, shape (n_samples_Y, n_features), (optional, default=None)\n",
    "            Right argument of the returned kernel k(X, Y). If None, k(X, X)\n",
    "            if evaluated instead.\n",
    "        eval_gradient : bool (optional, default=False)\n",
    "            Determines whether the gradient with respect to the kernel\n",
    "            hyperparameter is determined. Only supported when Y is None.\n",
    "        Returns\n",
    "        -------\n",
    "        K : array, shape (n_samples_X, n_samples_Y)\n",
    "            Kernel k(X, Y)\n",
    "        K_gradient : array (opt.), shape (n_samples_X, n_samples_X, n_dims)\n",
    "            The gradient of the kernel k(X, X) with respect to the\n",
    "            hyperparameter of the kernel. Only returned when eval_gradient\n",
    "            is True.\n",
    "        \"\"\"\n",
    "        X = np.atleast_2d(X)\n",
    "        length_scale = kernels._check_length_scale(X, self.length_scale)\n",
    "        if Y is None:\n",
    "            # JKH: All I changed was 'sqeuclidean' to 'euclidean'\n",
    "            dists = pdist(X / length_scale, metric='euclidean')\n",
    "            K = np.exp(-.5 * dists)\n",
    "            # convert from upper-triangular matrix to square matrix\n",
    "            K = squareform(K)\n",
    "            np.fill_diagonal(K, 1)\n",
    "        else:\n",
    "            if eval_gradient:\n",
    "                raise ValueError(\n",
    "                    \"Gradient can only be evaluated when Y is None.\")\n",
    "            dists = cdist(X / length_scale, Y / length_scale,\n",
    "                          metric='euclidean')\n",
    "            K = np.exp(-.5 * dists)\n",
    "\n",
    "        if eval_gradient:\n",
    "            if self.hyperparameter_length_scale.fixed:\n",
    "                # Hyperparameter l kept fixed\n",
    "                return K, np.empty((X.shape[0], X.shape[0], 0))\n",
    "            elif not self.anisotropic or length_scale.shape[0] == 1:\n",
    "                K_gradient = \\\n",
    "                    (K * squareform(dists))[:, :, np.newaxis]\n",
    "                return K, K_gradient\n",
    "            elif self.anisotropic:\n",
    "                # We need to recompute the pairwise dimension-wise distances\n",
    "                K_gradient = (X[:, np.newaxis, :] - X[np.newaxis, :, :]) ** 2 \\\n",
    "                    / (length_scale ** 2)\n",
    "                K_gradient *= K[..., np.newaxis]\n",
    "                return K, K_gradient\n",
    "        else:\n",
    "            return K\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.anisotropic:\n",
    "            return \"{0}(length_scale=[{1}])\".format(\n",
    "                self.__class__.__name__, \", \".join(map(\"{0:.3g}\".format,\n",
    "                                                   self.length_scale)))\n",
    "        else:  # isotropic\n",
    "            return \"{0}(length_scale={1:.3g})\".format(\n",
    "                self.__class__.__name__, np.ravel(self.length_scale)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below is the calibSpec_run code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Spectra Name: AGNData/NGC3783-_stitched_combined.fits\n",
      "Input Photometry Name: OzDES_Data/20190401_lc13_AGN.dat\n",
      "executing calibSpec\n",
      "Saving Data to AGNData/Output/test2NGC3783_scaled.fits\n",
      "Input Spectra Name: AGNData/IRAS09149-6206-_stitched_combined.fits\n",
      "Input Photometry Name: OzDES_Data/20190401_lc13_AGN.dat\n",
      "executing calibSpec\n",
      "Saving Data to AGNData/Output/test2IRAS09149-6206_scaled.fits\n"
     ]
    }
   ],
   "source": [
    "# %load OzDES_calibSpec_run.py\n",
    "# ---------------------------------------------------------- #\n",
    "# ----------------- OzDES_calibSpec_run.py ----------------- #\n",
    "# ------- https://github.com/jhoormann/OzDES_calibSpec ----- #\n",
    "# ---------------------------------------------------------- #\n",
    "# This is a code to perform spectrophotometric calibration.  #\n",
    "# It was designed to calibrate spectral data from the Anglo  #\n",
    "# Australian Telescope by matching it to near simultaneous   #\n",
    "# photometric observations using DECam on the Blanco         #\n",
    "# Telescope as part of the OzDES Reverberation Mapping       #\n",
    "# Program.   It also has the option to coadd all spectra     #\n",
    "# observed either by observing run or by date of observation.#\n",
    "# The bulk of the calculations are defined in the file       #\n",
    "# calibSpec_calc.py.  This code defines file locations,      #\n",
    "# reads in the data, and calls the calibration function.     #\n",
    "# Unless otherwise noted this code was written by            #\n",
    "# Janie Hoormann.                                            #\n",
    "# ---------------------------------------------------------- #\n",
    "import numpy as np\n",
    "#import OzDES_calibSpec_calc as calc\n",
    "\n",
    "# First define where all of the data can/will be found\n",
    "\n",
    "# Define where the transmission function is stored, the bands used, and the centers of each band\n",
    "bands = ['g', 'r', 'i']\n",
    "filters = {'g': 'OzDES_Data/DES_g_y3a1.dat',\n",
    "           'r': 'OzDES_Data/DES_r_y3a1.dat',\n",
    "           'i': 'OzDES_Data/DES_i_y3a1.dat'}\n",
    "centers = [4730, 6420, 7840]\n",
    "\n",
    "# Define where spectra are stored and file name format: name = spectraBase + ID + spectraEnd\n",
    "spectraBase = \"AGNData/\"#OzDES_Data/spectra180413/SVA1_COADD-\"\n",
    "spectraEnd = \"-_stitched_combined.fits\"\n",
    "\n",
    "# Define where photometry are stored and file name format\n",
    "photoBase = \"OzDES_Data/\"#photometryY5/\"\n",
    "photoEnd = \"1_lc12.dat\"\n",
    "\n",
    "# Define the name of the file that holds the list of sources to calibrate, which we want to be sure is an array\n",
    "# The OzDES IDs are 10 digit numbers so below, when the variable obj_name is defined it makes sure it was read in as an\n",
    "# integer and converted to a string.  If your IDs are different be sure to change that too!\n",
    "idNames = \"AGNData/WiFeS_AGN.txt\"\n",
    "names = np.genfromtxt(idNames, dtype = '|U16')\n",
    "\n",
    "if names.size == 1:\n",
    "    names = np.array([names])\n",
    "\n",
    "# Define the name of the place you want the output data stored\n",
    "outDir = \"AGNData/Output/\"\n",
    "\n",
    "# Do you want calibration plots - if so set the flag to the place where they should be saved, otherwise set it to false\n",
    "plotFlag = True\n",
    "plotFlag = \"AGNData/Output/\"\n",
    "\n",
    "# Do you want to coadd the spectra? If not the individual calibrated spectra will be save in a fits file\n",
    "# (coaddFlag == False), otherwise the spectra will be coadded based on the flag chosen (Date: Everything on same mjd\n",
    "# or Run: Everything on the same observing run)\n",
    "coaddFlag = False\n",
    "#coaddFlag = 'Date'\n",
    "#coaddFlag = 'Run'\n",
    "\n",
    "# When determining the DES photometric magnitudes at the same time of OzDES spectroscopic light curves the code normally\n",
    "# just linearly interpolates between the photometry.  This works fine because there is generally such high sampling.\n",
    "# However, if you have sparser data or what to forecast past when you have data you might want a more robust model.\n",
    "# You can then use a Gaussian Processes to fit a Brownian Bridge model to the data.  You are allowed to forecast out to\n",
    "# 28 days.  If you want to change this go to prevent_Excess.\n",
    "interpFlag = 'linear'\n",
    "# interpFlag = 'BBK'\n",
    "\n",
    "# You can also give a file with labeled columns ID and z so the redshift data can be saved with the\n",
    "# spectra. If you pass through False it will just be saved as -9.99\n",
    "#redshifts = False\n",
    "redshifts = \"AGNData/WiFeS_AGN_z.txt\"\n",
    "#redshift = [0.0127] #So you can manually input a redshift for a single source if you don't want to make a text file.\n",
    "\n",
    "# Now we actually call functions and do calculations\n",
    "for i in range(len(names)):\n",
    "    obj_name = str(names[i])#str(int(names[i]))\n",
    "\n",
    "    # Define input data names and read in spectra and photometric light curves\n",
    "    spectraName = spectraBase + obj_name + spectraEnd\n",
    "    photoName = 'OzDES_Data/20190401_lc13_AGN.dat' #photoBase + obj_name + photoEnd\n",
    "\n",
    "    print(\"Input Spectra Name: %s\" % spectraName)\n",
    "    spectra = Spectrumv18(spectraName)\n",
    "\n",
    "    # Clean up the spectra.  Marks large isolated large variations in flux and variance as bad (nan) and linearly\n",
    "    # interpolates over all nans\n",
    "    mark_as_bad(spectra.flux, spectra.variance)\n",
    "\n",
    "    print(\"Input Photometry Name: %s\" % photoName)\n",
    "\n",
    "    photo = np.loadtxt(photoName, dtype={'names':('nothing1','Date', 'Band', 'nothing2','nothing3', 'Mag', 'Mag_err','nothing4', 'nothing5'),\n",
    "                                         'formats':('|U4', np.float, '|U16', np.float, np.float, np.float, np.float, '|U7', '|U12')}, skiprows=1) \n",
    "    if redshifts != False:\n",
    "        zid, red = np.loadtxt(redshifts, unpack=True, skiprows=0, dtype = {'names':('zid','red'),'formats':('|U16',np.float)})\n",
    "\n",
    "        if obj_name in zid:\n",
    "            zi = np.where(zid == obj_name)\n",
    "            redshift = red[zi]\n",
    "        else:\n",
    "            redshift = [-9.99]\n",
    "\n",
    "    # Calls the main function which does the calibration\n",
    "    a = spectra.flux\n",
    "    calibSpec(obj_name, spectra, photo, spectraName, photoName, outDir, bands, filters, centers, plotFlag,\n",
    "                   coaddFlag, interpFlag, redshift)\n",
    "    b = spectra.flux\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
